{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "093ce007-6e4b-4444-9d75-ac13f15b118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "1a4e89c6-d873-4732-93a4-9328c25c1ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Age Sex ChestPainType  Cholesterol  FastingBS  MaxHR ExerciseAngina  \\\n",
      "0     40   M           ATA          289          0    172              N   \n",
      "1     49   F           NAP          180          0    156              N   \n",
      "2     37   M           ATA          283          0     98              N   \n",
      "3     48   F           ASY          214          0    108              Y   \n",
      "4     54   M           NAP          195          0    122              N   \n",
      "..   ...  ..           ...          ...        ...    ...            ...   \n",
      "913   45   M            TA          264          0    132              N   \n",
      "914   68   M           ASY          193          1    141              N   \n",
      "915   57   M           ASY          131          0    115              Y   \n",
      "916   57   F           ATA          236          0    174              N   \n",
      "917   38   M           NAP          175          0    173              N   \n",
      "\n",
      "     Oldpeak ST_Slope  HeartDisease  \n",
      "0        0.0       Up             0  \n",
      "1        1.0     Flat             1  \n",
      "2        0.0       Up             0  \n",
      "3        1.5     Flat             1  \n",
      "4        0.0       Up             0  \n",
      "..       ...      ...           ...  \n",
      "913      1.2     Flat             1  \n",
      "914      3.4     Flat             1  \n",
      "915      1.2     Flat             1  \n",
      "916      0.0     Flat             1  \n",
      "917      0.0       Up             0  \n",
      "\n",
      "[918 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('heart.csv')\n",
    "\n",
    "df = df.drop(['RestingBP', 'RestingECG'], axis=1)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "969a8526-6c62-43e0-8346-82deb851693a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[57 150 276 ... False True False]\n",
      " [62 140 394 ... False True False]\n",
      " [51 95 0 ... False True False]\n",
      " ...\n",
      " [49 130 341 ... False True False]\n",
      " [65 150 225 ... False True False]\n",
      " [43 150 247 ... False False True]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('heart.csv')\n",
    "df_one_hot_encoded = pd.get_dummies(df)\n",
    "\n",
    "# Assuming df_one_hot_encoded has features and target variable\n",
    "X = df_one_hot_encoded.drop(columns='HeartDisease')\n",
    "y = df_one_hot_encoded['HeartDisease']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\n",
    "\n",
    "x_train = x_train.values\n",
    "y_train = y_train.values\n",
    "x_test = x_test.values\n",
    "y_test = y_test.values\n",
    "\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "ef14a073-1d46-4118-8c34-874934920de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Class\n",
    "class decision_tree:\n",
    "    def __init__(self):\n",
    "        # 결정 트리 노드 속성 초기화\n",
    "        self.feature = None    # 분할에 사용할 특징 인덱스\n",
    "        self.value = None      # 분할에 사용할 임계값\n",
    "        self.left = None       # 왼쪽 서브트리\n",
    "        self.right = None      # 오른쪽 서브트리\n",
    "        self.label = None      # 리프 노드일 경우의 클래스 레이블\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        # 결정 트리를 재귀적으로 성장시킴\n",
    "        self.feature, self.value = self.find_best_split(x, y)\n",
    "\n",
    "        # 만약 최적 분할이 없다면 이 노드는 다수 클래스 레이블로 하는 리프 노드가 됨\n",
    "        if self.feature is None:\n",
    "            label, count = np.unique(y, return_counts=True)\n",
    "            self.label = label[np.argmax(count)]\n",
    "        else:\n",
    "            # 데이터를 분할하고 왼쪽과 오른쪽 서브트리를 재귀적으로 성장시킴\n",
    "            x_left, y_left, x_right, y_right = self.split_data(x, y, self.feature, self.value)\n",
    "            self.left = decision_tree()\n",
    "            self.right = decision_tree()\n",
    "            self.left.fit(x_left, y_left)\n",
    "            self.right.fit(x_right, y_right)\n",
    "\n",
    "    def find_best_split(self, x, y):\n",
    "        # Gini 불순도를 기반으로 최적 분할을 찾음\n",
    "        parent_gini = self.calculate_gini(y)\n",
    "        best_gini = parent_gini\n",
    "        best_feature, best_value = None, None\n",
    "\n",
    "        # 각 특징 및 해당 값의 고유값을 반복하여 최적 분할을 찾음\n",
    "        for column in range(x.shape[1]):\n",
    "            values = np.unique(x[:, column])\n",
    "            for value in values:\n",
    "                # 현재 특징과 값에 기반하여 데이터를 분할함\n",
    "                a, label_left, b, label_right = self.split_data(x, y, column, value)\n",
    "                gini = self.cost_function(label_left, label_right)\n",
    "\n",
    "                # 현재 분할이 더 낮은 Gini 불순도를 갖는다면 최적 분할을 업데이트함\n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_feature, best_value = column, value\n",
    "\n",
    "        return best_feature, best_value\n",
    "\n",
    "    def split_data(self, x, y, column, value):\n",
    "        # 주어진 특징과 값에 기반하여 데이터를 왼쪽과 오른쪽으로 분할함\n",
    "        left_mask = x[:, column] <= value\n",
    "        right_mask = x[:, column] > value\n",
    "        return x[left_mask], y[left_mask], x[right_mask], y[right_mask]\n",
    "\n",
    "    def calculate_gini(self, y):\n",
    "        # 레이블 집합에 대한 Gini 불순도를 계산함\n",
    "        labels, count = np.unique(y, return_counts=True)\n",
    "        p = count / count.sum()\n",
    "        gini = 1 - (p ** 2).sum()\n",
    "        return gini\n",
    "\n",
    "    def cost_function(self, y_left, y_right):\n",
    "        # Gini 불순도를 기반으로 분할의 비용을 계산함\n",
    "        num_total = len(y_left) + len(y_right)\n",
    "        p_left = len(y_left) / num_total\n",
    "        p_right = len(y_right) / num_total\n",
    "        cost = p_left * self.calculate_gini(y_left) + p_right * self.calculate_gini(y_right)\n",
    "        return cost\n",
    "\n",
    "    def predict(self, x):\n",
    "        # 단일 샘플에 대한 예측을 수행함\n",
    "        if self.label is not None:\n",
    "            # 리프 노드인 경우 클래스 레이블을 반환함\n",
    "            return self.label\n",
    "        else:\n",
    "            # 트리를 재귀적으로 탐색하여 예측 클래스를 찾음\n",
    "            if x[self.feature] <= self.value:\n",
    "                return self.left.predict(x)\n",
    "            else:\n",
    "                return self.right.predict(x)\n",
    "\n",
    "    def prediction(self, x):\n",
    "        # 다중 샘플에 대한 예측을 수행함\n",
    "        prediction = np.array([])\n",
    "        for row in x:\n",
    "            p = self.predict(row)\n",
    "            prediction = np.append(prediction, p)\n",
    "        return prediction\n",
    "\n",
    "    def accuracy(self, x, y):\n",
    "        # 주어진 데이터에 대한 모델의 정확도를 계산함\n",
    "        p = self.prediction(x)\n",
    "        compare = p == y\n",
    "        t_f, counts = np.unique(compare, return_counts=True)\n",
    "\n",
    "        # 정확도를 올바른 예측 수와 전체 예측 수의 비율로 계산함\n",
    "        acc = counts[np.where(t_f == True)] / len(y)\n",
    "        return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "a817008b-1a29-48f7-9351-f6a847c38b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree 결과\n",
      "train acc : 1.0\n",
      "test acc : 0.8260869565217391\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree 모델\n",
    "\n",
    "tree = decision_tree()\n",
    "\n",
    "tree.fit(x_train, y_train)\n",
    "train_acc = tree.accuracy(x_train, y_train)\n",
    "test_acc = tree.accuracy(x_test, y_test)\n",
    "\n",
    "print('Decision Tree 결과')\n",
    "print('train acc :', train_acc[0])\n",
    "print('test acc :', test_acc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "43901564-8de0-47ce-956b-28dc366ced21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Class\n",
    "class random_forest:\n",
    "    def __init__(self, n):\n",
    "        # 랜덤 포레스트 객체 초기화\n",
    "        self.n = n\n",
    "        self.trees = [] # 의사결정 트리를 저장할 리스트\n",
    "\n",
    "    def bagging(self, x_train, y_train):\n",
    "        # Bagging을 수행하여 랜덤 포레스트 훈련\n",
    "        for i in range(self.n):\n",
    "            index = len(x_train)\n",
    "            random_index = np.random.choice(index, index, True)\n",
    "            random_x_train = x_train[random_index]\n",
    "            random_y_train = y_train[random_index]\n",
    "\n",
    "            tree = decision_tree()  # 의사결정 트리 객체 생성\n",
    "            tree.fit(random_x_train, random_y_train)  # 의사결정 트리 훈련\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def rf_predict(self, x_test):\n",
    "        # 랜덤 포레스트로 예측 결과 결합\n",
    "        predictions = []\n",
    "        for tree in self.trees:\n",
    "            p = tree.prediction(x_test)\n",
    "            predictions.append(p)\n",
    "\n",
    "        # 각 트리의 예측 결과를 평균하여 최종 예측 결과 계산\n",
    "        prediction = np.mean(predictions, axis=0)\n",
    "        return np.round(prediction)  # 이진 분류를 가정하고 있으므로 반올림하여 0 또는 1로 변환\n",
    "\n",
    "    def rf_accuracy(self, x_test, y_test):\n",
    "        # 랜덤 포레스트의 정확도 계산\n",
    "        p = self.rf_predict(x_test)\n",
    "        compare = p == y_test\n",
    "        t_f, counts = np.unique(compare, return_counts=True)\n",
    "        acc = counts[np.where(t_f == True)] / len(y_test)\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "0b64b3e9-ad07-4f58-afc6-dc6858acec4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest 결과\n",
      "train acc : 0.9918256130790191\n",
      "test acc : 0.9021739130434783\n"
     ]
    }
   ],
   "source": [
    "# Random Forest 모델\n",
    "\n",
    "forest = random_forest(10)\n",
    "\n",
    "forest.bagging(x_train, y_train)\n",
    "train_acc = forest.rf_accuracy(x_train, y_train)\n",
    "test_acc = forest.rf_accuracy(x_test, y_test)\n",
    "\n",
    "print('Random Forest 결과')\n",
    "print('train acc :', train_acc[0])\n",
    "print('test acc :', test_acc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "de3fb499-7465-454e-b331-8840de380a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoosting:\n",
    "    def __init__(self, n_estimators=10, learning_rate=0.1):\n",
    "        # GradientBoosting 객체 초기화\n",
    "        '''\n",
    "        n_estimators : 사용할 의사결정 트리의 개수\n",
    "        learning_rate : 학습률\n",
    "        '''\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.pred = 0\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # 초기 예측값은 로지스틱 함수 적용\n",
    "        initial_prediction = np.log(np.mean(y) / (1 - np.mean(y)))\n",
    "        prediction = initial_prediction\n",
    "        self.pred = np.log(np.mean(y))\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            # pseudo residual 계산\n",
    "            residual = y - np.exp(prediction) / (1 + np.exp(prediction))\n",
    "\n",
    "            # 의사결정 트리 생성\n",
    "            tree = decision_tree()\n",
    "            tree.fit(X, residual)\n",
    "\n",
    "            prediction += self.learning_rate * tree.prediction(X)\n",
    "\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        prediction = self.pred\n",
    "\n",
    "        # 각 트리의 예측 결과를 학습률과 곱하여 현재 예측값에 더함\n",
    "        for tree in self.trees:\n",
    "            prediction += self.learning_rate * tree.prediction(X)\n",
    "\n",
    "        # x>0 ? 1 : 0 으로 이진 분류\n",
    "        return (prediction > 0).astype(int)\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        p = self.predict(X)\n",
    "        acc = np.mean(p == y)\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "e2803f36-47a8-4091-8a7b-142d97cd5f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting 결과\n",
      "train acc : 1.0\n",
      "test acc : 0.8260869565217391\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting 모델\n",
    "\n",
    "GBM = GradientBoosting(n_estimators=10, learning_rate=0.5)\n",
    "\n",
    "GBM.fit(x_train, y_train)\n",
    "train_acc = GBM.accuracy(x_train, y_train)\n",
    "test_acc = GBM.accuracy(x_test, y_test)\n",
    "\n",
    "print('Gradient Boosting 결과')\n",
    "print('train acc :', train_acc)\n",
    "print('test acc :', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f377dbf1-d24c-4b54-9f1c-dfc0b1062099",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
